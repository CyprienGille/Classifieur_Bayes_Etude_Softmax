{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maths du projet\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définitions argmax softmax\n",
    "$$\n",
    "argmax:\n",
    "\\begin{align}\n",
    "\\mathbb{R}^n & \\to \\{ 0,1 \\} ^n \\\\\n",
    " V = [v_i] & \\mapsto W = [w_i]\\ avec\\ \n",
    "\\begin{cases}\n",
    "w_i =& 1\\ si\\ v_i=\\underset{i}{max}(V)\\\\\n",
    "& 0\\ sinon\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "softmax:\n",
    "\\begin{align}\n",
    "\\mathbb{R}^n & \\to [0,1]^n \\\\\n",
    " V = [v_i] & \\mapsto W = [w_i]\\ avec\\ w_i = \\frac{\\exp(v_{i})} {\\sum_{k=1}^{n} \\exp(v_{k})}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition Classifieur\n",
    "Si l'on note $\\mathcal{D}$ un classifieur, $X$ une donnée (aussi appelée observation) et $Y_k, k\\in [1, K]$ les $K$ différentes classes, une classification se représente donc ainsi:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}(X) = Y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "D^* = \\underset{\\mathcal{D} \\in \\mathbf{D}}{argmin} \\; R(\\mathcal{D})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} = \\underset{k \\in \\{1, \\ldots, K\\}}{argmax} \\ p(Y = k) \\prod_{i=1}^n p(X_i \\mid Y = k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition Risque\n",
    "Le risque dépend du classifieur utilisé (argmax ou softmax) et de la répartition des données dans les classes. Le classifieur bayésien (avec argmax) reste noté $\\mathcal{D}^*$, et on note le classifieur softmax $\\mathcal{\\tilde{D}}$. \n",
    "\n",
    "On note $\\pi$ le vecteur contenant les probabilités a priori: $\\pi_i = P(Y=i)$\n",
    "\n",
    "L'expression du risque est alors:\n",
    "$$\n",
    "R(\\mathcal{D}, \\pi) = \\sum_{i \\in \\{1, \\ldots, K\\}} \\pi_i R_i(\\mathcal{D})\n",
    "$$\n",
    "avec $R_i$ les risques d'erreur conditionnels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition Matrices Probas et Décision\n",
    "\n",
    "Notons $N$ le nombre d'observations (i.e. de données), $K$ restant le nombre de classes. Notre matrice de probabilités $A'$ est alors de dimension $(K, N)$:\n",
    "\n",
    "$$\n",
    "A' = \n",
    "\\begin{pmatrix}\n",
    "P(Y=1 \\mid X_1) & \\dots & P(Y=1 \\mid X_N) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "P(Y=K \\mid X_1) & \\dots & P(Y=K \\mid X_N)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "On applique le théorème de Bayes, et on obtient la matrice $A$ des probabilités postérieures, où apparaît la première dépendance en $\\pi$.\n",
    "$$\n",
    "A = \n",
    "\\begin{pmatrix}\n",
    "P(X_1 \\mid Y=1)\\pi_1 & \\dots & P(X_N \\mid Y=1)\\pi_1 \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "P(X_1 \\mid Y=K)\\pi_k & \\dots & P(X_N \\mid Y=K)\\pi_k\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi,\n",
    "$$\n",
    "a_{ij} = \\pi_i P(X_j \\mid Y=i)  \\\\\n",
    "i \\in \\{1 \\ldots K \\}, j\\in \\{1 \\ldots N\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D star\n",
    "L'application d'argmax se fait selon les colonnes:\n",
    "$$\n",
    "D^*_j = \\underset{i}{argmax}[A_j]\n",
    "$$\n",
    "Ce qui donne donc:\n",
    "$$\n",
    "d_{ij}^* = \\left( \\underset{k}{argmax} P(X_j \\mid Y=k)\\pi_k \\right) _i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D tilde\n",
    "\n",
    "$$\n",
    "\\tilde{D}_j = softmax[A_j]\n",
    "$$\n",
    "Ce qui donne donc:\n",
    "$$\n",
    "\\tilde{d}_{ij} = \\frac{\\exp(\\pi_i P(X_j \\mid Y=i))} {\\sum_{k=1}^{K} \\exp(\\pi_k P(X_j \\mid Y=k))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "R_i (D) &= \\sum_{j} \\sum_{k\\ne i} P(Y = k|X_j)\\\\\n",
    "&=  \\sum_{j} C^i D\n",
    "\\end{align}\n",
    "$$\n",
    "Avec $C^i$ un vecteur $(1, K)$ tel que :\n",
    "$$\n",
    "C^i_j = \n",
    "\\begin{cases}\n",
    "1\\ si\\ j \\ne i\\\\\n",
    "0\\ sinon\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risque Argmax\n",
    "$$\n",
    "\\begin{align}\n",
    "R(D^*, \\pi) &= \\sum_{i \\in \\{1, \\ldots, K\\}} \\pi_i R_i(D^*)\\\\\n",
    "&= \\sum_{i} \\sum_{j} \\pi_i C^i D^* \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risque Softmax\n",
    "$$\n",
    "\\begin{align}\n",
    "R(\\tilde{D}, \\pi) &= \\sum_{i \\in \\{1, \\ldots, K\\}} \\pi_i R_i(\\tilde{D})\\\\\n",
    "&= \\sum_{i} \\sum_{j} \\pi_i C^i \\tilde{D}\\\\\n",
    "&= \\sum_{i} \\sum_{j} \\pi_i C^i_j \\tilde{d}_{ij}\\\\\n",
    "R(\\tilde{D}, \\pi) &= \\sum_{i=1}^K \\sum_{j=1}^N \\pi_i (1 - \\delta_{ij}) \\frac{\\exp(\\pi_i P(X_j \\mid Y=i))} {\\sum_{k=1}^{K} \\exp(\\pi_k P(X_j \\mid Y=k))}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dérivées de softmax\n",
    "Si l'on note :\n",
    "$$\n",
    "S_i = \\frac{\\exp(a_i)} {\\sum_{k=1}^{K} \\exp(a_k)}\n",
    "$$\n",
    "Alors on a:\n",
    "$$\n",
    "\\frac{\\partial S_i}{\\partial a_j} = \n",
    "\\begin{cases}\n",
    "S_i (1 - S_j)\\ si\\ i=j\\\\\n",
    "-S_j S_i \\ si\\ i\\ne j\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient du risque softmax\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial R(\\tilde{D}, \\pi)}{\\partial \\pi_i} &= \\frac{\\partial}{\\partial \\pi_i} \\sum_{i=1}^K \\pi_i \\sum_{j=1}^N  (1 - \\delta_{ij}) \\frac{\\exp(\\pi_i P(X_j \\mid Y=i))} {\\sum_{k=1}^{K} \\exp(\\pi_k P(X_j \\mid Y=k))}\\\\\n",
    "&= \\frac{\\partial}{\\partial \\pi_i} \\sum_{i=1}^K \\pi_i \\sum_{j=1}^N  (1 - \\delta_{ij}) S_i\\\\\n",
    "&= \\sum_{j=1}^{N} (1 - \\delta_{ij}) S_i \n",
    "+ \\pi_i \\sum_{j=1}^{N} (1 - \\delta_{ij}) \\frac{\\partial S_i}{\\partial \\pi_i} \n",
    "+ \\sum_{k \\ne i}^{K} \\sum_{j=1}^{N} (1 - \\delta_{kj}) \\frac{\\partial S_k}{\\partial \\pi_i} \\\\\n",
    "&= \\sum_{j=1}^{N} (1 - \\delta_{ij}) S_i\n",
    "+ \\pi_i \\sum_{j=1}^{N} (1 - \\delta_{ij}) S_i (1 - S_i)\n",
    "+ \\sum_{k \\ne i}^{K} \\sum_{j=1}^{N} (1 - \\delta_{kj}) (-S_k S_i)\\\\\n",
    "&= \\sum_{j=1}^{N} \\left[ (1 - \\delta_{ij}) S_i\n",
    "+ \\pi_i (1 - \\delta_{ij}) S_i (1 - S_i)\n",
    "+ \\sum_{k \\ne i}^{K} (1 - \\delta_{kj}) (-S_k S_i) \\right]\\\\\n",
    "\\frac{\\partial R(\\tilde{D}, \\pi)}{\\partial \\pi_i} &= \\sum_{j=1}^{N} \\left[ (1 - \\delta_{ij}) \\left( S_i\n",
    "+ \\pi_i S_i (1 - S_i) \\right)\n",
    "+ \\sum_{k \\ne i}^{K} (1 - \\delta_{kj}) (-S_k S_i) \\right]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Démo concavité risque argmax\n",
    "Posons $V(\\pi) = R(D^*, \\pi)$ le risque d'erreur global pour des probabilités a priori $\\pi$.\n",
    "\n",
    "Soit $\\alpha \\in [0, 1]$, soient $\\pi$ et $\\pi'$ deux probabilités à priori, et $\\pi''$ une troisième telle que\n",
    "$\\pi'' = \\alpha \\pi + (1-\\alpha) \\pi'$\n",
    "\n",
    "On a alors, en vertu de la minimisation du risque par le classifieur de Bayes argmax:\n",
    "$$\n",
    "\\begin{align}\n",
    "V(\\pi'') &= \\alpha \\pi^T R(D^*, \\pi'') + (1 - \\alpha) \\pi'^T R(D^*, \\pi'')\\\\\n",
    "&\\geq \\alpha V(\\pi) + (1 - \\alpha) V(\\pi')\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection simplex\n",
    "Le point du K-simplex le plus proche de $\\pi_i$ est:\n",
    "$$\n",
    "t_i = \\underset{i}{max} \\{\\pi_i - \\Delta_i, 0 \\}\n",
    "$$\n",
    "Avec :\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Delta_i &= \\frac{\\left( \\sum_{j=i+1}^{K} \\pi_j \\right) - 1}{K - i}\n",
    "\\end{align}\n",
    "$$\n",
    "Avec les $\\pi_i$ triés par ordre croissant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
